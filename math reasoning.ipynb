{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Mohammad Mahdi Razmjoo\n",
    "\n",
    "Student Number: 400101272"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce-oIWvr7q0Y"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:53:57.016555Z",
     "iopub.status.busy": "2025-03-28T16:53:57.016189Z",
     "iopub.status.idle": "2025-03-28T16:53:57.022178Z",
     "shell.execute_reply": "2025-03-28T16:53:57.021160Z",
     "shell.execute_reply.started": "2025-03-28T16:53:57.016527Z"
    },
    "id": "VQ7TDsNa7uJw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:54:06.610222Z",
     "iopub.status.busy": "2025-03-28T16:54:06.609856Z",
     "iopub.status.idle": "2025-03-28T16:54:11.081870Z",
     "shell.execute_reply": "2025-03-28T16:54:11.080861Z",
     "shell.execute_reply.started": "2025-03-28T16:54:06.610192Z"
    },
    "id": "KI5_NNwaD4ol",
    "outputId": "8fe48d61-961b-43ff-e95d-dfef1b7878c2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:54:16.271507Z",
     "iopub.status.busy": "2025-03-28T16:54:16.271166Z",
     "iopub.status.idle": "2025-03-28T16:54:16.275516Z",
     "shell.execute_reply": "2025-03-28T16:54:16.274653Z",
     "shell.execute_reply.started": "2025-03-28T16:54:16.271475Z"
    },
    "id": "iSdhU2OgD9-G",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPHkjJ0E6GAj"
   },
   "source": [
    "# Configurations & Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:54:36.018114Z",
     "iopub.status.busy": "2025-03-28T16:54:36.017757Z",
     "iopub.status.idle": "2025-03-28T16:54:36.023497Z",
     "shell.execute_reply": "2025-03-28T16:54:36.022489Z",
     "shell.execute_reply.started": "2025-03-28T16:54:36.018084Z"
    },
    "id": "ZIja0mSe6SeU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Research the Mistral-7B-Instruct-v0.3 model (capabilities, use cases, limitations)\n",
    "model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# TODO: Research Hugging Face API token.\n",
    "hugging_face_token: str = \"#############\"\n",
    "\n",
    "# The directory path to save our intermediate results.\n",
    "output_dir: str = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# TODO: Research the impact of max_new_tokens, do_sample, and temperature on model outputs.\n",
    "max_new_tokens: int = 256 \n",
    "do_sample: bool = False \n",
    "temperature: float = 1.0\n",
    "\n",
    "# The number of data points we choose for our mini project.\n",
    "sample_numbers: int = 50 # you can adjust it to 100 if you have more GPU, the accuracy becomes better.\n",
    "\n",
    "# The number of shots for few-shot prompting (default is 2).\n",
    "shots: int = 2\n",
    "\n",
    "# The number of generated outputs in the self-consistency setting.\n",
    "K: int = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:54:39.280167Z",
     "iopub.status.busy": "2025-03-28T16:54:39.279794Z",
     "iopub.status.idle": "2025-03-28T16:54:39.285383Z",
     "shell.execute_reply": "2025-03-28T16:54:39.284255Z",
     "shell.execute_reply.started": "2025-03-28T16:54:39.280137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Functions to write and read data in a JSON file (for storing intermediate outputs).\n",
    "\n",
    "def write_json(data, filename):\n",
    "    \"\"\"\n",
    "    Writes data to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data (dict or list): The data to be written to the file.\n",
    "        filename (str): The path to the JSON file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "\n",
    "def read_json(filename):\n",
    "    \"\"\"\n",
    "    Reads and loads data from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict or list: The parsed JSON content.\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:54:41.803726Z",
     "iopub.status.busy": "2025-03-28T16:54:41.803405Z",
     "iopub.status.idle": "2025-03-28T16:54:41.808075Z",
     "shell.execute_reply": "2025-03-28T16:54:41.807179Z",
     "shell.execute_reply.started": "2025-03-28T16:54:41.803699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to read content from a .txt file (used for reading demonstrations)\n",
    "\n",
    "def read_from_txt(file_path):\n",
    "    \"\"\"\n",
    "    Reads the entire content of a .txt file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the file as a string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:54:45.744978Z",
     "iopub.status.busy": "2025-03-28T16:54:45.744626Z",
     "iopub.status.idle": "2025-03-28T16:54:45.750464Z",
     "shell.execute_reply": "2025-03-28T16:54:45.749481Z",
     "shell.execute_reply.started": "2025-03-28T16:54:45.744951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Functions to extract an answer from generated text and postprocess the final answer\n",
    "def extract_answer(generated_text):\n",
    "    \"\"\"\n",
    "    Extracts the last numerical expression from the generated text.\n",
    "\n",
    "    Args:\n",
    "        generated_text (str): The text output from which to extract a numeric answer.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The extracted numerical expression if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Compile a regex pattern to find both integers and floats (positive or negative)\n",
    "    pattern = re.compile(r'[-+]?\\d*\\.\\d+|\\d+')\n",
    "    \n",
    "    # Find all matches in the generated text\n",
    "    matches = pattern.findall(generated_text)\n",
    "    \n",
    "    # If there are matches, return the last one; otherwise return None\n",
    "    if matches:\n",
    "        return matches[-1]\n",
    "    return None\n",
    "\n",
    "def postprocess_final_answer(numeric_expression):\n",
    "    \"\"\"\n",
    "    Cleans and evaluates a numeric expression to obtain a final answer.\n",
    "\n",
    "    Args:\n",
    "        numeric_expression (str): A string containing a numeric expression.\n",
    "\n",
    "    Returns:\n",
    "        str: The evaluated result as a string if computation is successful,\n",
    "             otherwise returns the original input.\n",
    "\n",
    "    Method:\n",
    "    1. Removes commas from the numeric expression to avoid formatting issues.\n",
    "    2. Attempts to evaluate the expression using Python's `eval()`.\n",
    "    3. If evaluation fails (e.g., due to invalid input), returns the original expression.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaned_up = numeric_expression.replace(',', '')  # Remove commas for proper evaluation\n",
    "        result = eval(cleaned_up)  # Evaluate the numeric expression\n",
    "        return str(result)  # Convert result to string and return\n",
    "    except Exception:\n",
    "        return numeric_expression  # Return the original expression if evaluation fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:54:48.672030Z",
     "iopub.status.busy": "2025-03-28T16:54:48.671721Z",
     "iopub.status.idle": "2025-03-28T16:54:48.677383Z",
     "shell.execute_reply": "2025-03-28T16:54:48.676340Z",
     "shell.execute_reply.started": "2025-03-28T16:54:48.672007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def aggregate_paths_based_on_scores(paths):\n",
    "    \"\"\"\n",
    "    Aggregates answer paths based on their confidence scores and selects the best answer.\n",
    "\n",
    "    Args:\n",
    "        paths (list of tuples): Each tuple contains (answer, confidence, final_answer).\n",
    "            - answer: The full answer text (str)\n",
    "            - confidence: A numerical score representing confidence (float/int)\n",
    "            - final_answer: The final extracted answer (str)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_full_ans, best_score, best_answer)\n",
    "               - best_full_ans: The full answer with highest total confidence.\n",
    "               - best_score: The aggregated confidence score of the best answer.\n",
    "               - best_answer: The final answer with the highest overall confidence.\n",
    "    \"\"\"\n",
    "    # Dictionary to sum confidence scores for each final answer\n",
    "    answer_scores = {}\n",
    "    \n",
    "    # Dictionary to store an example full answer for each final answer\n",
    "    answer_text_map = {}\n",
    "\n",
    "    # Accumulate confidence for each unique final_answer\n",
    "    for full_answer, confidence, final_answer in paths:\n",
    "        if final_answer not in answer_scores:\n",
    "            answer_scores[final_answer] = 0\n",
    "            # Store the first or most representative full_answer\n",
    "            answer_text_map[final_answer] = full_answer\n",
    "        answer_scores[final_answer] += confidence\n",
    "    \n",
    "    # Identify the final answer with the highest confidence sum\n",
    "    best_answer = max(answer_scores, key=answer_scores.get)\n",
    "    \n",
    "    # Retrieve the aggregated confidence score and the corresponding full answer\n",
    "    best_score = answer_scores[best_answer]\n",
    "    best_full_ans = answer_text_map[best_answer]\n",
    "    \n",
    "    return best_full_ans, best_score, best_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:54:52.630975Z",
     "iopub.status.busy": "2025-03-28T16:54:52.630643Z",
     "iopub.status.idle": "2025-03-28T16:54:52.635863Z",
     "shell.execute_reply": "2025-03-28T16:54:52.634818Z",
     "shell.execute_reply.started": "2025-03-28T16:54:52.630947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(json_path):\n",
    "    \"\"\"\n",
    "    Computes the accuracy based on a JSON file containing correctness labels.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing the evaluation data.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy, calculated as the proportion of correctly classified items.\n",
    "    \"\"\"\n",
    "    data = read_json(json_path)\n",
    "    \n",
    "    total = len(data)\n",
    "    if total == 0:\n",
    "        return 0.0  # Avoid divide-by-zero if somehow the file is empty\n",
    "    \n",
    "    # Count how many items have 'is_correct' == True\n",
    "    correct = sum(1 for item in data if item.get('is_correct') == True)\n",
    "    \n",
    "    # Return the fraction of items that are correct\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV7fW2dn6BeM"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:55:01.199952Z",
     "iopub.status.busy": "2025-03-28T16:55:01.199512Z",
     "iopub.status.idle": "2025-03-28T16:55:01.206109Z",
     "shell.execute_reply": "2025-03-28T16:55:01.205124Z",
     "shell.execute_reply.started": "2025-03-28T16:55:01.199914Z"
    },
    "id": "lCJpmQy55gI2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: research about math reasoning datasets, especially the GSM8K.\n",
    "def load_gsm8k(sample_number):\n",
    "    \"\"\"\n",
    "    Loads a subset of the GSM8K dataset for evaluation.\n",
    "\n",
    "    Args:\n",
    "        sample_number (int): The number of samples to select from the test set.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: A subset of the GSM8K test dataset with the specified number of samples.\n",
    "\n",
    "    Method:\n",
    "    1. Loads the \"openai/gsm8k\" dataset from the Hugging Face Datasets library.\n",
    "    2. Selects the 'test' split of the dataset.\n",
    "    3. Extracts the first `sample_number` examples from the test set.\n",
    "    4. Returns the selected subset.\n",
    "    \n",
    "    Note:\n",
    "        Ensure that `sample_number` does not exceed the total number of test samples.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"openai/gsm8k\", 'main')['test']\n",
    "    dataset_samples = dataset.select(range(sample_number))\n",
    "    return dataset_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:55:08.984689Z",
     "iopub.status.busy": "2025-03-28T16:55:08.984370Z",
     "iopub.status.idle": "2025-03-28T16:55:11.638793Z",
     "shell.execute_reply": "2025-03-28T16:55:11.638057Z",
     "shell.execute_reply.started": "2025-03-28T16:55:08.984667Z"
    },
    "id": "X7Pe_0nsDlK4",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36420e868ecb45a0aa09d3b146929a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2125f780c14fabb626e6a2c9dbcebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350a8cf6a54a4047a03fb4bef8e46313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abad002ce8744ac6904ac469983ed3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bc3bc60ee14a5796622223471d76ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gsm8k = load_gsm8k(sample_number= sample_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgQegqCe6TV9"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:55:22.812656Z",
     "iopub.status.busy": "2025-03-28T16:55:22.812131Z",
     "iopub.status.idle": "2025-03-28T16:55:22.819390Z",
     "shell.execute_reply": "2025-03-28T16:55:22.818297Z",
     "shell.execute_reply.started": "2025-03-28T16:55:22.812630Z"
    },
    "id": "suBuQNwe6URg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: research about the Mistral model and its capabilities.\n",
    "# TODO: research about device_map and trasnformers.pipeline.\n",
    "class LLM:\n",
    "    \"\"\"\n",
    "    A wrapper class for a large language model (LLM) using the Hugging Face Transformers library.\n",
    "\n",
    "    Attributes:\n",
    "        model_id (str): The identifier of the model to be loaded.\n",
    "        max_new_tokens (int): The maximum number of tokens the model should generate.\n",
    "        do_sample (bool): Whether to use sampling when generating text.\n",
    "        temperature (float): Controls randomness in text generation.\n",
    "        llm (transformers.pipeline): The loaded text-generation model.\n",
    "\n",
    "    Methods:\n",
    "        load_llm(model_id):\n",
    "            Loads the specified language model with optimized settings.\n",
    "        \n",
    "        generate(system_prompt, user_prompt):\n",
    "            Generates a response based on a system prompt and user input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_id, do_sample, temperature, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Initializes the LLM class and loads the model.\n",
    "\n",
    "        Args:\n",
    "            model_id (str): The Hugging Face model identifier.\n",
    "            do_sample (bool): Enables stochastic text generation when True.\n",
    "            temperature (float): Adjusts randomness in text generation.\n",
    "            max_new_tokens (int): Defines the maximum length of generated responses.\n",
    "        \"\"\"\n",
    "        self.model_id = model_id\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.do_sample = do_sample\n",
    "        self.temperature = temperature\n",
    "        self.llm = self.load_llm(model_id)\n",
    "\n",
    "    def load_llm(self, model_id):\n",
    "        \"\"\"\n",
    "        Loads the text-generation model using the Hugging Face Transformers pipeline.\n",
    "\n",
    "        Args:\n",
    "            model_id (str): The model identifier from Hugging Face.\n",
    "\n",
    "        Returns:\n",
    "            transformers.pipeline: A text-generation pipeline with optimized settings.\n",
    "        \"\"\"\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_id,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},  # Use bfloat16 for memory efficiency\n",
    "            device_map=\"auto\",  # Automatically assigns model to the best available device (GPU/CPU)\n",
    "        )\n",
    "        return pipeline\n",
    "\n",
    "    def generate(self, system_prompt, user_prompt):\n",
    "        \"\"\"\n",
    "        Generates a response from the model based on a system prompt and user input.\n",
    "\n",
    "        Args:\n",
    "            system_prompt (str): A system-level instruction to guide the model's response.\n",
    "            user_prompt (str): The user's query or input.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response from the model.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "\n",
    "        outputs = self.llm(\n",
    "            messages,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            do_sample=self.do_sample,\n",
    "            temperature=self.temperature,\n",
    "            pad_token_id=self.llm.tokenizer.eos_token_id  # Ensure proper padding\n",
    "        )\n",
    "\n",
    "        return outputs[0][\"generated_text\"][-1]['content']  # Extracts the last generated content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:55:27.438493Z",
     "iopub.status.busy": "2025-03-28T16:55:27.438157Z",
     "iopub.status.idle": "2025-03-28T16:55:27.528125Z",
     "shell.execute_reply": "2025-03-28T16:55:27.527368Z",
     "shell.execute_reply.started": "2025-03-28T16:55:27.438471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "login(hugging_face_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:55:31.132721Z",
     "iopub.status.busy": "2025-03-28T16:55:31.132396Z",
     "iopub.status.idle": "2025-03-28T16:58:48.190413Z",
     "shell.execute_reply": "2025-03-28T16:58:48.189545Z",
     "shell.execute_reply.started": "2025-03-28T16:55:31.132697Z"
    },
    "id": "K_nRdSLzDyVl",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7404f1857444e79acaa411bc9d19e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cd9a500c5845878807aad51bf37e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0006571b50846549067753f3cd6ee92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f697faf74942455991480c30a9e8e8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fd06a30dae4bd6ba0ac258b39f5438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a7b1c553af4be9b01dd7f970e467fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c79136341af4e99b38a6c723990dbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94428255f7fb45f997947edc91e619d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21fb47caf4b407a84809aa4cbacb8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc04a48f92e4f229722af5670021161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781e406b44df42ed955bab35890e02f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cb31c62b5c41b2800376a4d2fb0a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model_id=model_id, do_sample=do_sample,\n",
    "          temperature=temperature, max_new_tokens=max_new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ANMPj4FEEo8"
   },
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyTSqxqZEHAn"
   },
   "source": [
    "## Direct Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:59:15.806226Z",
     "iopub.status.busy": "2025-03-28T16:59:15.805463Z",
     "iopub.status.idle": "2025-03-28T16:59:15.814291Z",
     "shell.execute_reply": "2025-03-28T16:59:15.813018Z",
     "shell.execute_reply.started": "2025-03-28T16:59:15.806176Z"
    },
    "id": "vdgv7lquEGDP",
    "outputId": "ee17b8d2-1b4b-43e4-82c4-f53cfdab76b7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def direct_prompt(llm: LLM, system_prompt, dataset, save_path):\n",
    "    \"\"\"\n",
    "    Runs direct prompting on a given dataset using an LLM and evaluates accuracy.\n",
    "\n",
    "    Args:\n",
    "        llm (LLM): The language model instance used for generating responses.\n",
    "        system_prompt (str): The system-level instruction for guiding the model.\n",
    "        dataset (Dataset): A dataset containing questions and their ground-truth answers.\n",
    "        save_path (str): The file path where the results will be saved in JSON format.\n",
    "\n",
    "    Returns:\n",
    "        None: Results are saved to `save_path`.\n",
    "\n",
    "    Process:\n",
    "    1. Iterates through each question in the dataset.\n",
    "    2. Extracts the ground-truth answer and generates a model response.\n",
    "    3. Post-processes the generated answer and compares it with the ground truth.\n",
    "    4. Stores the results, including correctness, in a list.\n",
    "    5. Writes intermediate results to a JSON file at every iteration.\n",
    "    6. Computes and displays a running accuracy score using a progress bar.\n",
    "    \"\"\"\n",
    "    total_questions = len(dataset)\n",
    "    results = [] \n",
    "\n",
    "    with tqdm(total=total_questions, dynamic_ncols=True) as pbar:\n",
    "        for query_id, data in enumerate(dataset):\n",
    "            question = data['question']\n",
    "            ground_truth_numeric = extract_answer(data['answer'])\n",
    "\n",
    "            generated_text = llm.generate(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=question\n",
    "            )\n",
    "\n",
    "            generated_numeric = extract_answer(generated_text)\n",
    "            final_answer = (\n",
    "                postprocess_final_answer(generated_numeric)\n",
    "                if generated_numeric is not None\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            is_correct = (final_answer == ground_truth_numeric)\n",
    "\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"model_answer\": generated_text,\n",
    "                \"generated_numeric\": generated_numeric,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"ground_truth\": ground_truth_numeric,\n",
    "                \"is_correct\": is_correct\n",
    "            })\n",
    "\n",
    "            write_json(results, save_path)\n",
    "\n",
    "            correct_answers = sum(1 for r in results if r[\"is_correct\"])\n",
    "            running_accuracy = (correct_answers / len(results)) * 100\n",
    "            pbar.set_postfix(idx=query_id + 1, running_accuracy=f\"{running_accuracy:.2f}%\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    write_json(results, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:04:41.631076Z",
     "iopub.status.busy": "2025-03-28T17:04:41.630711Z",
     "iopub.status.idle": "2025-03-28T17:30:58.452944Z",
     "shell.execute_reply": "2025-03-28T17:30:58.452172Z",
     "shell.execute_reply.started": "2025-03-28T17:04:41.631024Z"
    },
    "id": "o-LXPzgTFGNU",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating system prompt variant: concise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [01:11<05:02,  7.56s/it, idx=10, running_accuracy=40.00%]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 50/50 [05:15<00:00,  6.31s/it, idx=50, running_accuracy=24.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Variant 'concise' Accuracy: 24.00%\n",
      "\n",
      "Evaluating system prompt variant: step_by_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [10:29<00:00, 12.58s/it, idx=50, running_accuracy=18.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Variant 'step_by_step' Accuracy: 18.00%\n",
      "\n",
      "Evaluating system prompt variant: brief_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:01<00:00,  1.23s/it, idx=50, running_accuracy=14.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Variant 'brief_final' Accuracy: 14.00%\n",
      "\n",
      "Evaluating system prompt variant: detailed_reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [09:30<00:00, 11.41s/it, idx=50, running_accuracy=34.00%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Variant 'detailed_reasoning' Accuracy: 34.00%\n",
      "\n",
      "All system prompt evaluation results:\n",
      "concise: 24.00%\n",
      "step_by_step: 18.00%\n",
      "brief_final: 14.00%\n",
      "detailed_reasoning: 34.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# I made a slight change to this part of the code and combined two previous sections for better evaluation\n",
    "system_prompts = {\n",
    "    \"concise\": \"Provide a direct and concise answer to the question without additional commentary.\",\n",
    "    \"step_by_step\": \"Answer the question step-by-step, showing your reasoning before giving the final answer.\",\n",
    "    \"brief_final\": \"Respond briefly with only the final answer, without any elaboration or additional explanation.\",\n",
    "    \"detailed_reasoning\": \"Think through the problem carefully and then provide a succinct answer with key reasoning steps highlighted.\"\n",
    "}\n",
    "\n",
    "variant_accuracies = {}\n",
    "for variant_name, prompt in system_prompts.items():\n",
    "    print(f\"Evaluating system prompt variant: {variant_name}\")\n",
    "    save_path_variant = output_dir / f\"direct_prompt_{variant_name}.json\"\n",
    "    \n",
    "    direct_prompt(llm=llm, system_prompt=prompt, dataset=gsm8k, save_path=save_path_variant)\n",
    "    \n",
    "    accuracy_variant = get_accuracy(save_path_variant)\n",
    "    variant_accuracies[variant_name] = accuracy_variant\n",
    "    print(f\"System Prompt Variant '{variant_name}' Accuracy: {accuracy_variant:.2%}\\n\")\n",
    "\n",
    "print(\"All system prompt evaluation results:\")\n",
    "for variant, acc in variant_accuracies.items():\n",
    "    print(f\"{variant}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sz0cDUWxEJYE"
   },
   "source": [
    "## Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:40:45.718979Z",
     "iopub.status.busy": "2025-03-28T17:40:45.718652Z",
     "iopub.status.idle": "2025-03-28T17:40:45.725604Z",
     "shell.execute_reply": "2025-03-28T17:40:45.724834Z",
     "shell.execute_reply.started": "2025-03-28T17:40:45.718954Z"
    },
    "id": "fRuZhSWFEM6b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def zero_shot(llm: LLM, system_prompt, cot_prefix, dataset, save_path):\n",
    "    total_questions = len(dataset)\n",
    "    results = [] \n",
    "\n",
    "    with tqdm(total=total_questions, dynamic_ncols=True) as pbar:\n",
    "        for query_id, data in enumerate(dataset):\n",
    "            question = data[\"question\"]\n",
    "            ground_truth = extract_answer(data[\"answer\"])\n",
    "\n",
    "            constructed_prompt = f\"{question}\\n{cot_prefix}\\nAnswer:\"\n",
    "            generated_text = llm.generate(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=constructed_prompt\n",
    "            )\n",
    "\n",
    "            generated_numeric = extract_answer(generated_text)\n",
    "            final_answer = postprocess_final_answer(generated_numeric) if generated_numeric else None\n",
    "            is_correct = (final_answer == ground_truth)\n",
    "\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"prompt_used\": constructed_prompt,\n",
    "                \"model_answer\": generated_text,\n",
    "                \"generated_numeric\": generated_numeric,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"is_correct\": is_correct\n",
    "            })\n",
    "\n",
    "            write_json(results, save_path)\n",
    "\n",
    "            correct_answers = sum(1 for r in results if r[\"is_correct\"])\n",
    "            running_accuracy = (correct_answers / (query_id + 1)) * 100\n",
    "\n",
    "            pbar.set_postfix(idx=query_id + 1, running_accuracy=f\"{running_accuracy:.2f}%\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    write_json(results, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:40:52.535386Z",
     "iopub.status.busy": "2025-03-28T17:40:52.535078Z",
     "iopub.status.idle": "2025-03-28T18:10:25.039606Z",
     "shell.execute_reply": "2025-03-28T18:10:25.038720Z",
     "shell.execute_reply.started": "2025-03-28T17:40:52.535362Z"
    },
    "id": "rWbzcpLOFbN1",
    "outputId": "bc3d2f3f-4f48-4bc2-9c71-8c335d23e986",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CoT prefix variant: step_by_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [09:42<00:00, 11.66s/it, idx=50, running_accuracy=40.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT Variant 'step_by_step' Accuracy: 40.00%\n",
      "\n",
      "Evaluating CoT prefix variant: reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [09:33<00:00, 11.47s/it, idx=50, running_accuracy=34.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT Variant 'reasoning' Accuracy: 34.00%\n",
      "\n",
      "Evaluating CoT prefix variant: detailed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [10:15<00:00, 12.32s/it, idx=50, running_accuracy=36.00%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT Variant 'detailed' Accuracy: 36.00%\n",
      "\n",
      "All CoT prefix evaluation results:\n",
      "step_by_step: 40.00%\n",
      "reasoning: 34.00%\n",
      "detailed: 36.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# I made a slight change to this part of the code and combined two previous sections for better evaluation\n",
    "cot_prefix_variants = {\n",
    "    \"step_by_step\": \"let's think step by step\",\n",
    "    \"reasoning\": \"Let's carefully reason through the problem and solve it:\",\n",
    "    \"detailed\": \"Begin by analyzing the problem step by step before arriving at the final answer:\"\n",
    "}\n",
    "\n",
    "cot_variant_accuracies = {}\n",
    "\n",
    "for variant_name, cot_prefix in cot_prefix_variants.items():\n",
    "    print(f\"Evaluating CoT prefix variant: {variant_name}\")\n",
    "    \n",
    "    save_path_variant = output_dir / f\"zero_shot_{variant_name}.json\"\n",
    "    \n",
    "    zero_shot(\n",
    "        llm=llm,\n",
    "        system_prompt=\"Be helpful, answer the question.\",\n",
    "        cot_prefix=cot_prefix,\n",
    "        dataset=gsm8k,\n",
    "        save_path=save_path_variant\n",
    "    )\n",
    "    \n",
    "    accuracy_variant = get_accuracy(save_path_variant)\n",
    "    cot_variant_accuracies[variant_name] = accuracy_variant\n",
    "    print(f\"CoT Variant '{variant_name}' Accuracy: {accuracy_variant:.2%}\\n\")\n",
    "\n",
    "print(\"All CoT prefix evaluation results:\")\n",
    "for variant, acc in cot_variant_accuracies.items():\n",
    "    print(f\"{variant}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv8kVTOCENaI"
   },
   "source": [
    "## Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm6-GaGcEjKl"
   },
   "source": [
    "### Few-shot Prompting with Correct Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T18:32:32.377858Z",
     "iopub.status.busy": "2025-03-28T18:32:32.377517Z",
     "iopub.status.idle": "2025-03-28T18:32:32.386156Z",
     "shell.execute_reply": "2025-03-28T18:32:32.384925Z",
     "shell.execute_reply.started": "2025-03-28T18:32:32.377833Z"
    },
    "id": "AgUKX83OEOtc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def few_shot(llm, system_prompt, shots, dataset, save_path):\n",
    "    \"\"\"\n",
    "    Performs few-shot prompting on a dataset using an LLM and evaluates accuracy.\n",
    "\n",
    "    Args:\n",
    "        llm (LLM): The language model instance used for generating responses.\n",
    "        system_prompt (str): The system-level instruction guiding the model.\n",
    "        shots (int): The number of few-shot examples to use.\n",
    "        dataset (Dataset): A dataset containing questions and their ground-truth answers.\n",
    "        save_path (str): The file path where results will be saved in JSON format.\n",
    "\n",
    "    Returns:\n",
    "        None: Results are saved to `save_path`.\n",
    "\n",
    "    Process:\n",
    "    1. Loads pre-written few-shot demonstrations from a text file.\n",
    "    2. Iterates through each question in the dataset.\n",
    "    3. Formats the prompt by inserting the question into the demonstration template.\n",
    "    4. Extracts the ground-truth answer and generates a model response.\n",
    "    5. Post-processes the generated answer and compares it with the ground truth.\n",
    "    6. Stores the results, including correctness, in a list.\n",
    "    7. Writes intermediate results to a JSON file at every iteration.\n",
    "    8. Computes and displays a running accuracy score using a progress bar.\n",
    "    \"\"\"\n",
    "    total_questions = len(dataset)\n",
    "    results = []\n",
    "\n",
    "    demonstrations = read_from_txt(f\"/kaggle/input/few-shot-dataset/{shots}_shots.txt\")\n",
    "\n",
    "    with tqdm(total=total_questions, dynamic_ncols=True) as pbar:\n",
    "        for query_id, data in enumerate(dataset):\n",
    "            question = data['question']\n",
    "            ground_truth = extract_answer(data['answer'])\n",
    "\n",
    "            user_prompt = f\"{demonstrations}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "            generated_text = llm.generate(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt\n",
    "            )\n",
    "\n",
    "            generated_numeric = extract_answer(generated_text)\n",
    "            final_answer = (postprocess_final_answer(generated_numeric)\n",
    "                            if generated_numeric is not None\n",
    "                            else None)\n",
    "\n",
    "            is_correct = (final_answer == ground_truth)\n",
    "\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"demonstrations\": demonstrations,\n",
    "                \"model_answer\": generated_text,\n",
    "                \"generated_numeric\": generated_numeric,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"is_correct\": is_correct\n",
    "            })\n",
    "\n",
    "            write_json(results, save_path)\n",
    "\n",
    "            correct_answers = sum(1 for r in results if r[\"is_correct\"])\n",
    "            running_accuracy = (correct_answers / (query_id + 1)) * 100\n",
    "            pbar.set_postfix(idx=query_id + 1, running_accuracy=f\"{running_accuracy:.2f}%\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    write_json(results, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T18:32:37.850664Z",
     "iopub.status.busy": "2025-03-28T18:32:37.850306Z",
     "iopub.status.idle": "2025-03-28T19:02:44.161662Z",
     "shell.execute_reply": "2025-03-28T19:02:44.160826Z",
     "shell.execute_reply.started": "2025-03-28T18:32:37.850635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating few_shot with 2 shots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [08:34<00:00, 10.29s/it, idx=50, running_accuracy=40.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few_shot with 2 shots, accuracy: 40.00%\n",
      "\n",
      "Evaluating few_shot with 4 shots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [09:47<00:00, 11.75s/it, idx=50, running_accuracy=40.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few_shot with 4 shots, accuracy: 40.00%\n",
      "\n",
      "Evaluating few_shot with 8 shots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [11:44<00:00, 14.08s/it, idx=50, running_accuracy=36.00%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few_shot with 8 shots, accuracy: 36.00%\n",
      "\n",
      "Few-shot evaluation results:\n",
      "2 shots: 40.00%\n",
      "4 shots: 40.00%\n",
      "8 shots: 36.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# I made a slight change to this part of the code and combined two previous sections for better evaluation\n",
    "shots_list = [2, 4, 8]\n",
    "shots_accuracy = {}\n",
    "\n",
    "for shots in shots_list:\n",
    "    print(f\"Evaluating few_shot with {shots} shots...\")\n",
    "    \n",
    "    few_shot_save_path = output_dir / f\"few_shot_{shots}.json\"\n",
    "    \n",
    "    few_shot(\n",
    "        llm=llm,\n",
    "        system_prompt=\"Be helpful, answer the question.\",\n",
    "        shots=shots,\n",
    "        dataset=gsm8k,\n",
    "        save_path=few_shot_save_path\n",
    "    )\n",
    "    \n",
    "    accuracy = get_accuracy(few_shot_save_path)\n",
    "    shots_accuracy[shots] = accuracy\n",
    "    print(f\"Few_shot with {shots} shots, accuracy: {accuracy:.2%}\\n\")\n",
    "\n",
    "print(\"Few-shot evaluation results:\")\n",
    "for shot_count, acc in shots_accuracy.items():\n",
    "    print(f\"{shot_count} shots: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4PAzmxtEcbX"
   },
   "source": [
    "### Few-shot Prompting with Wrong Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T19:13:01.616338Z",
     "iopub.status.busy": "2025-03-28T19:13:01.615948Z",
     "iopub.status.idle": "2025-03-28T19:13:01.623825Z",
     "shell.execute_reply": "2025-03-28T19:13:01.622854Z",
     "shell.execute_reply.started": "2025-03-28T19:13:01.616307Z"
    },
    "id": "N224SVd2EmGn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def wrong_shot(llm, system_prompt, shots, dataset, save_path):\n",
    "    \"\"\"\n",
    "    Performs few-shot prompting using incorrect demonstrations (negative examples)\n",
    "    and evaluates the impact on model performance.\n",
    "\n",
    "    Args:\n",
    "        llm (LLM): The language model instance used for generating responses.\n",
    "        system_prompt (str): The system-level instruction guiding the model.\n",
    "        shots (int): The number of incorrect few-shot examples to use.\n",
    "        dataset (Dataset): A dataset containing questions and their ground-truth answers.\n",
    "        save_path (str): The file path where results will be saved in JSON format.\n",
    "\n",
    "    Returns:\n",
    "        None: Results are saved to `save_path`.\n",
    "\n",
    "    Process:\n",
    "    1. Loads incorrect few-shot demonstrations from a text file (`negative_{shots}_shots.txt`).\n",
    "    2. Iterates through each question in the dataset.\n",
    "    3. Formats the prompt by inserting the question into the incorrect demonstration template.\n",
    "    4. Extracts the ground-truth answer and generates a model response.\n",
    "    5. Post-processes the generated answer and compares it with the ground truth.\n",
    "    6. Stores the results, including correctness, in a list.\n",
    "    7. Writes intermediate results to a JSON file at every iteration.\n",
    "    8. Computes and displays a running accuracy score using a progress bar.\n",
    "    \"\"\"\n",
    "    total_questions = len(dataset)\n",
    "    results = []\n",
    "\n",
    "    demonstrations = read_from_txt(f\"/kaggle/input/few-negative-shot/negative_{shots}_shots.txt\")\n",
    "\n",
    "    with tqdm(total=total_questions, dynamic_ncols=True) as pbar:\n",
    "        for query_id, data in enumerate(dataset):\n",
    "            question = data['question']\n",
    "            ground_truth = extract_answer(data['answer'])\n",
    "\n",
    "            user_prompt = f\"{demonstrations}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "            generated_text = llm.generate(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt\n",
    "            )\n",
    "\n",
    "            generated_numeric = extract_answer(generated_text)\n",
    "            final_answer = (postprocess_final_answer(generated_numeric)\n",
    "                            if generated_numeric is not None\n",
    "                            else None)\n",
    "\n",
    "            is_correct = (final_answer == ground_truth)\n",
    "\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"demonstrations\": demonstrations,\n",
    "                \"model_answer\": generated_text,\n",
    "                \"generated_numeric\": generated_numeric,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"is_correct\": is_correct\n",
    "            })\n",
    "\n",
    "            write_json(results, save_path)\n",
    "\n",
    "            correct_answers = sum(1 for r in results if r[\"is_correct\"])\n",
    "            running_accuracy = (correct_answers / (query_id + 1)) * 100\n",
    "\n",
    "            pbar.set_postfix(idx=query_id + 1, running_accuracy=f\"{running_accuracy:.2f}%\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    write_json(results, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T19:13:33.883261Z",
     "iopub.status.busy": "2025-03-28T19:13:33.882891Z",
     "iopub.status.idle": "2025-03-28T19:41:39.030290Z",
     "shell.execute_reply": "2025-03-28T19:41:39.029403Z",
     "shell.execute_reply.started": "2025-03-28T19:13:33.883231Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wrong_shot with 2 shots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [08:53<00:00, 10.66s/it, idx=50, running_accuracy=34.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_shot with 2 shots, accuracy: 34.00%\n",
      "\n",
      "Evaluating wrong_shot with 4 shots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [09:06<00:00, 10.92s/it, idx=50, running_accuracy=34.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_shot with 4 shots, accuracy: 34.00%\n",
      "\n",
      "Evaluating wrong_shot with 8 shots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [10:05<00:00, 12.12s/it, idx=50, running_accuracy=42.00%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_shot with 8 shots, accuracy: 42.00%\n",
      "\n",
      "Wrong-shot evaluation results:\n",
      "2 shots: 34.00%\n",
      "4 shots: 34.00%\n",
      "8 shots: 42.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# I made a slight change to this part of the code and combined two previous sections for better evaluation\n",
    "wrong_shots_list = [2, 4, 8]\n",
    "wrong_shot_accuracies = {}\n",
    "\n",
    "for shots in wrong_shots_list:\n",
    "    print(f\"Evaluating wrong_shot with {shots} shots...\")\n",
    "    \n",
    "    wrong_shot_save_path = output_dir / f\"wrong_shot_{shots}.json\"\n",
    "    \n",
    "    wrong_shot(\n",
    "        llm=llm,\n",
    "        system_prompt=\"Be helpful, answer the question.\",\n",
    "        shots=shots,\n",
    "        dataset=gsm8k,\n",
    "        save_path=wrong_shot_save_path\n",
    "    )\n",
    "    \n",
    "    accuracy = get_accuracy(wrong_shot_save_path)\n",
    "    wrong_shot_accuracies[shots] = accuracy\n",
    "    print(f\"wrong_shot with {shots} shots, accuracy: {accuracy:.2%}\\n\")\n",
    "\n",
    "print(\"Wrong-shot evaluation results:\")\n",
    "for shot_count, acc in wrong_shot_accuracies.items():\n",
    "    print(f\"{shot_count} shots: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hK84ByDHEQgE"
   },
   "source": [
    "## Self-Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T19:45:11.750194Z",
     "iopub.status.busy": "2025-03-28T19:45:11.749826Z",
     "iopub.status.idle": "2025-03-28T19:45:11.759721Z",
     "shell.execute_reply": "2025-03-28T19:45:11.758883Z",
     "shell.execute_reply.started": "2025-03-28T19:45:11.750163Z"
    },
    "id": "cWOiI_bmERog",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def self_consistency(llm, system_prompt, K, dataset, save_path):\n",
    "    \"\"\"\n",
    "    Implements the Self-Consistency method for answer generation.\n",
    "    \n",
    "    Instead of relying on a single response, this function generates K different responses\n",
    "    for each question and selects the most frequently occurring answer as the final prediction.\n",
    "    This approach aims to improve accuracy and robustness.\n",
    "\n",
    "    Args:\n",
    "        llm (LLM): The language model instance for text generation.\n",
    "        system_prompt (str): The system-level instruction guiding the model.\n",
    "        K (int): The number of independent generations for self-consistency.\n",
    "        dataset (Dataset): The dataset containing questions and ground-truth answers.\n",
    "        save_path (str): Path to store results in a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        None: Results are saved to `save_path`.\n",
    "\n",
    "    Process:\n",
    "    1. Iterates through each question in the dataset.\n",
    "    2. Generates K independent responses for the same question.\n",
    "    3. Extracts and post-processes answers from the generated text.\n",
    "    4. Selects the most frequently occurring answer (majority voting).\n",
    "    5. Compares the selected answer with the ground truth.\n",
    "    6. Stores results in a list and updates a progress bar with running accuracy.\n",
    "    7. Writes results progressively to prevent data loss.\n",
    "    \"\"\"\n",
    "    total_questions = len(dataset)\n",
    "    results = []\n",
    "\n",
    "    with tqdm(total=total_questions, dynamic_ncols=True) as pbar:\n",
    "        for query_id, data in enumerate(dataset):\n",
    "            question = data['question']\n",
    "            ground_truth = extract_answer(data['answer'])\n",
    "\n",
    "            generated_texts = []\n",
    "            numeric_answers = []\n",
    "\n",
    "            for _ in range(K):\n",
    "                response_text = llm.generate(\n",
    "                    system_prompt=system_prompt,\n",
    "                    user_prompt=question\n",
    "                )\n",
    "                generated_texts.append(response_text)\n",
    "\n",
    "                numeric_ans = extract_answer(response_text)\n",
    "                if numeric_ans is not None:\n",
    "                    final_ans = postprocess_final_answer(numeric_ans)\n",
    "                else:\n",
    "                    final_ans = None\n",
    "\n",
    "                numeric_answers.append(final_ans)\n",
    "\n",
    "            freq_counter = Counter(numeric_answers)\n",
    "            if None in freq_counter and len(freq_counter) > 1:\n",
    "                freq_counter.pop(None)\n",
    "\n",
    "            best_answer, _ = freq_counter.most_common(1)[0] if freq_counter else (None, 0)\n",
    "\n",
    "            is_correct = (best_answer == ground_truth)\n",
    "\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"model_answers\": generated_texts,    \n",
    "                \"numeric_answers\": numeric_answers,\n",
    "                \"best_answer\": best_answer,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"is_correct\": is_correct\n",
    "            })\n",
    "\n",
    "            write_json(results, save_path)\n",
    "\n",
    "            correct_answers = sum(1 for r in results if r[\"is_correct\"])\n",
    "            running_accuracy = (correct_answers / (query_id + 1)) * 100\n",
    "\n",
    "            pbar.set_postfix(idx=query_id + 1, running_accuracy=f\"{running_accuracy:.2f}%\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    write_json(results, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T19:45:21.867065Z",
     "iopub.status.busy": "2025-03-28T19:45:21.866744Z",
     "iopub.status.idle": "2025-03-28T21:08:38.776908Z",
     "shell.execute_reply": "2025-03-28T21:08:38.775995Z",
     "shell.execute_reply.started": "2025-03-28T19:45:21.867019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running self_consistency with K = 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [20:41<00:00, 24.82s/it, idx=50, running_accuracy=30.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_consistency with K = 3, accuracy: 30.00%\n",
      "\n",
      "Running self_consistency with K = 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [27:31<00:00, 33.03s/it, idx=50, running_accuracy=30.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_consistency with K = 4, accuracy: 30.00%\n",
      "\n",
      "Running self_consistency with K = 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [35:04<00:00, 42.08s/it, idx=50, running_accuracy=30.00%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_consistency with K = 5, accuracy: 30.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# I made a slight change to this part of the code and combined two previous sections for better evaluation\n",
    "for current_K in [3, 4, 5]:\n",
    "    print(f\"Running self_consistency with K = {current_K}...\")\n",
    "    self_consistency_save_path = output_dir / f\"self_consistency_{current_K}.json\"\n",
    "    llm.temperature = 1.0\n",
    "\n",
    "    self_consistency(\n",
    "        llm=llm,\n",
    "        system_prompt=\"Be helpful and answer the question concisely.\",\n",
    "        K=current_K,\n",
    "        dataset=gsm8k,\n",
    "        save_path=self_consistency_save_path\n",
    "    )\n",
    "\n",
    "    accuracy = get_accuracy(self_consistency_save_path)\n",
    "    print(f\"self_consistency with K = {current_K}, accuracy: {accuracy:.2%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXPPHm0ZESED"
   },
   "source": [
    "## Verbalized Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T21:10:53.507142Z",
     "iopub.status.busy": "2025-03-28T21:10:53.506781Z",
     "iopub.status.idle": "2025-03-28T21:10:53.517896Z",
     "shell.execute_reply": "2025-03-28T21:10:53.516853Z",
     "shell.execute_reply.started": "2025-03-28T21:10:53.507111Z"
    },
    "id": "KSgeHfDWEUec",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def verbalized_confidence(llm, system_prompt, K, dataset, save_path):\n",
    "    \"\"\"\n",
    "    Generates answers to a set of questions using a language model (LLM) and assigns a confidence score \n",
    "    to each generated response. Selects the most confident response and evaluates its correctness.\n",
    "\n",
    "    Args:\n",
    "        llm: The language model used for generating responses.\n",
    "        system_prompt (str): The initial system prompt to guide the model.\n",
    "        K (int): Number of answer candidates to generate per question.\n",
    "        dataset (list): List of dictionaries, each containing a question and its corresponding answer.\n",
    "        save_path (str): Path to save the results as a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        None (Results are saved to a JSON file).\n",
    "    \"\"\"\n",
    "\n",
    "    total_questions = len(dataset)\n",
    "    results = []\n",
    "\n",
    "    with tqdm(total=total_questions, dynamic_ncols=True) as pbar:\n",
    "        for query_id, data in enumerate(dataset):\n",
    "            question = data[\"question\"]\n",
    "            ground_truth = extract_answer(data[\"answer\"])\n",
    "\n",
    "            candidate_answers = []\n",
    "            candidate_confidences = []\n",
    "            candidate_texts = []\n",
    "\n",
    "            for _ in range(K):\n",
    "                answer_prompt = (\n",
    "                    f\"{question}\\n\"\n",
    "                    \"Let's reason this out step-by-step, then give a final numeric answer.\\n\"\n",
    "                    \"Answer:\"\n",
    "                )\n",
    "                generated_answer_text = llm.generate(\n",
    "                    system_prompt=system_prompt,\n",
    "                    user_prompt=answer_prompt\n",
    "                )\n",
    "\n",
    "                numeric_ans = extract_answer(generated_answer_text)\n",
    "                final_ans = (\n",
    "                    postprocess_final_answer(numeric_ans) if numeric_ans is not None else None\n",
    "                )\n",
    "\n",
    "                confidence_prompt = (\n",
    "                    f\"I provided the answer '{final_ans}'. \"\n",
    "                    \"On a scale from 0 to 100, how confident am I in this answer? \"\n",
    "                    \"Please only respond with the number.\"\n",
    "                )\n",
    "                generated_confidence_text = llm.generate(\n",
    "                    system_prompt=system_prompt,\n",
    "                    user_prompt=confidence_prompt\n",
    "                )\n",
    "\n",
    "                conf_match = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", generated_confidence_text)\n",
    "                if conf_match:\n",
    "                    try:\n",
    "                        raw_conf = float(conf_match[-1])\n",
    "                        confidence_value = max(0.0, min(100.0, raw_conf))\n",
    "                    except ValueError:\n",
    "                        confidence_value = 0.0\n",
    "                else:\n",
    "                    confidence_value = 0.0\n",
    "\n",
    "                candidate_answers.append(final_ans)\n",
    "                candidate_confidences.append(confidence_value)\n",
    "                candidate_texts.append({\n",
    "                    \"answer_text\": generated_answer_text,\n",
    "                    \"confidence_text\": generated_confidence_text\n",
    "                })\n",
    "\n",
    "            if candidate_confidences:\n",
    "                max_index = max(range(len(candidate_confidences)), key=candidate_confidences.__getitem__)\n",
    "                best_answer = candidate_answers[max_index]\n",
    "                best_confidence = candidate_confidences[max_index]\n",
    "            else:\n",
    "                best_answer = None\n",
    "                best_confidence = 0.0\n",
    "\n",
    "            is_correct = (best_answer == ground_truth)\n",
    "\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"candidate_answers\": candidate_answers,\n",
    "                \"candidate_confidences\": candidate_confidences,\n",
    "                \"candidate_texts\": candidate_texts,\n",
    "                \"best_answer\": best_answer,\n",
    "                \"best_confidence\": best_confidence,\n",
    "                \"is_correct\": is_correct\n",
    "            })\n",
    "\n",
    "            write_json(results, save_path)\n",
    "\n",
    "            correct_answers = sum(1 for r in results if r[\"is_correct\"])\n",
    "            running_accuracy = (correct_answers / (query_id + 1)) * 100\n",
    "\n",
    "            pbar.set_postfix(idx=query_id + 1, running_accuracy=f\"{running_accuracy:.2f}%\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    write_json(results, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T21:11:20.692247Z",
     "iopub.status.busy": "2025-03-28T21:11:20.691918Z",
     "iopub.status.idle": "2025-03-28T23:35:50.667803Z",
     "shell.execute_reply": "2025-03-28T23:35:50.666615Z",
     "shell.execute_reply.started": "2025-03-28T21:11:20.692220Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verbalized_confidence with K = 3 answer candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [36:44<00:00, 44.09s/it, idx=50, running_accuracy=36.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbalized_confidence with K = 3, accuracy: 36.00%\n",
      "\n",
      "Running verbalized_confidence with K = 4 answer candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [48:20<00:00, 58.00s/it, idx=50, running_accuracy=36.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbalized_confidence with K = 4, accuracy: 36.00%\n",
      "\n",
      "Running verbalized_confidence with K = 5 answer candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [59:25<00:00, 71.31s/it, idx=50, running_accuracy=36.00%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbalized_confidence with K = 5, accuracy: 36.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# I made a slight change to this part of the code and combined two previous sections for better evaluation\n",
    "verbalized_confidence_results = {}\n",
    "\n",
    "for current_K in [3, 4, 5]:\n",
    "    print(f\"Running verbalized_confidence with K = {current_K} answer candidates...\")\n",
    "    \n",
    "    save_path = output_dir / f\"verbalized_confidence_{current_K}.json\"\n",
    "    llm.do_sample = True  \n",
    "    llm.temperature = 1.0  \n",
    "\n",
    "    verbalized_confidence(\n",
    "        llm=llm,\n",
    "        system_prompt=\"Be helpful, answer the question.\",\n",
    "        K=current_K,\n",
    "        dataset=gsm8k,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    accuracy = get_accuracy(save_path)\n",
    "    verbalized_confidence_results[current_K] = accuracy\n",
    "    print(f\"verbalized_confidence with K = {current_K}, accuracy: {accuracy:.2%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZF1BF3LEU94"
   },
   "source": [
    "## Subquestion Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T14:40:42.799398Z",
     "iopub.status.busy": "2025-03-27T14:40:42.799027Z",
     "iopub.status.idle": "2025-03-27T14:40:42.808333Z",
     "shell.execute_reply": "2025-03-27T14:40:42.807013Z",
     "shell.execute_reply.started": "2025-03-27T14:40:42.799369Z"
    },
    "id": "QhHIvhjGEZJt",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def subquestion(llm, system_prompt, dataset, save_path):\n",
    "    \"\"\"\n",
    "    Decomposes complex questions into a series of subquestions (few-shot style) and\n",
    "    then solves them step-by-step, returning a final numeric answer.\n",
    "\n",
    "    We provide a short demonstration for how to break down a math word problem\n",
    "    into simpler subquestions, then answer them in a second step with chain-of-thought.\n",
    "\n",
    "    Args:\n",
    "        llm: The language model used for generating responses.\n",
    "        system_prompt (str): The initial system prompt to guide the model's general behavior.\n",
    "        dataset (list): A list of dictionaries, each containing a question and its corresponding answer.\n",
    "        save_path (str): Path to save the results as a JSON file.\n",
    "    \"\"\"\n",
    "    total_questions = len(dataset)\n",
    "    results = []\n",
    "\n",
    "    llm.do_sample = True  \n",
    "    llm.temperature = 0.7\n",
    "\n",
    "    decomposition_demo = \"\"\"\\\n",
    "**Example Decomposition**:\n",
    "Question: \"Tom has 3 apples and 2 oranges. If each fruit costs 2 dollars, how much money does Tom need?\"\n",
    "Subquestions:\n",
    "1) How many total fruits does Tom have?\n",
    "2) What is the cost for each fruit?\n",
    "3) What is the total cost for all fruits?\n",
    "\n",
    "---\n",
    "\n",
    "Question: \"Sally runs 2 miles every day for 7 days. How many miles does she run in total?\"\n",
    "Subquestions:\n",
    "1) How many days does Sally run?\n",
    "2) How many miles does she run each day?\n",
    "3) What is the total after 7 days?\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "    solving_demo = \"\"\"\\\n",
    "**Example Solving**:\n",
    "Subquestions:\n",
    "1) How many total fruits does Tom have?\n",
    "Answer: 3 + 2 = 5\n",
    "2) What is the cost for each fruit?\n",
    "Answer: 2 dollars per fruit\n",
    "3) What is the total cost for all fruits?\n",
    "Answer: 5 fruits * 2 dollars = 10\n",
    "\n",
    "Final numeric answer: 10\n",
    "\n",
    "---\n",
    "\n",
    "Subquestions:\n",
    "1) How many days does Sally run?\n",
    "Answer: 7\n",
    "2) How many miles does she run each day?\n",
    "Answer: 2\n",
    "3) What is the total after 7 days?\n",
    "Answer: 7 * 2 = 14\n",
    "\n",
    "Final numeric answer: 14\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "    with tqdm(total=total_questions, dynamic_ncols=True) as pbar:\n",
    "        for query_id, data in enumerate(dataset):\n",
    "            question = data[\"question\"]\n",
    "            ground_truth = extract_answer(data[\"answer\"])\n",
    "\n",
    "            decomposition_prompt = (\n",
    "                f\"{decomposition_demo}\"\n",
    "                f\"Now decompose the following question into simpler subquestions:\\n\\n\"\n",
    "                f\"Question:\\n{question}\\n\\n\"\n",
    "                \"Subquestions:\\n\"\n",
    "            )\n",
    "            subquestions_text = llm.generate(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=decomposition_prompt\n",
    "            )\n",
    "\n",
    "            solving_prompt = (\n",
    "                f\"{solving_demo}\\n\"\n",
    "                f\"Now solve these subquestions step by step:\\n\"\n",
    "                f\"{subquestions_text}\\n\\n\"\n",
    "                \"Finally, provide the numeric answer at the end.\\nAnswer:\"\n",
    "            )\n",
    "            solving_text = llm.generate(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=solving_prompt\n",
    "            )\n",
    "\n",
    "            final_answer_raw = extract_answer(solving_text)\n",
    "            final_answer = postprocess_final_answer(final_answer_raw) if final_answer_raw else None\n",
    "            is_correct = (final_answer == ground_truth)\n",
    "\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"subquestions_text\": subquestions_text,\n",
    "                \"solving_text\": solving_text,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"is_correct\": is_correct\n",
    "            })\n",
    "\n",
    "            write_json(results, save_path)\n",
    "\n",
    "            correct_answers = sum(1 for r in results if r[\"is_correct\"])\n",
    "            running_accuracy = (correct_answers / (query_id + 1)) * 100\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                idx=query_id + 1,\n",
    "                running_accuracy=f\"{running_accuracy:.2f}%\"\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "    write_json(results, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T14:40:53.674514Z",
     "iopub.status.busy": "2025-03-27T14:40:53.674167Z",
     "iopub.status.idle": "2025-03-27T14:59:24.703808Z",
     "shell.execute_reply": "2025-03-27T14:59:24.703011Z",
     "shell.execute_reply.started": "2025-03-27T14:40:53.674483Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [18:31<00:00, 22.22s/it, idx=50, running_accuracy=4.00%]\n"
     ]
    }
   ],
   "source": [
    "subquestion_save_path = output_dir / \"subquestion.json\"\n",
    "\n",
    "llm.do_sample = False  \n",
    "llm.temperature = 1.0  \n",
    "\n",
    "subquestion(\n",
    "    llm, \n",
    "    system_prompt=\"Be helpful, answer the question.\",  \n",
    "    dataset=gsm8k, \n",
    "    save_path=subquestion_save_path \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T15:00:01.316720Z",
     "iopub.status.busy": "2025-03-27T15:00:01.316416Z",
     "iopub.status.idle": "2025-03-27T15:00:01.322270Z",
     "shell.execute_reply": "2025-03-27T15:00:01.321424Z",
     "shell.execute_reply.started": "2025-03-27T15:00:01.316699Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: subquestion, accuracy: 0.04\n"
     ]
    }
   ],
   "source": [
    "json_path = output_dir / f\"subquestion.json\"\n",
    "accuracy = get_accuracy(json_path)\n",
    "print(f\"method: subquestion, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "**TODO:** Compare all the algorithms implemented in the notebook in your report, highlighting which ones align with your expectations and which ones do not. Provide a detailed analysis of your observations, explaining the reasons behind each outcome. For instance, Why advanced methods fail to perform effectively."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6886437,
     "sourceId": 11053456,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6982221,
     "sourceId": 11185292,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6982314,
     "sourceId": 11185421,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6982419,
     "sourceId": 11185573,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6992471,
     "sourceId": 11199472,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6992648,
     "sourceId": 11199710,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
